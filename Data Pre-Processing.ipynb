{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1r_BOtj9UidBwfZR77ChqknHzipp6vA8x","authorship_tag":"ABX9TyMuILksnShE+Y6DOvawyrNq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data Cleansing"],"metadata":{"id":"JgNewikYJJcK"}},{"cell_type":"markdown","source":["In this stage, I implemented an automated data cleansing pipeline to preprocess raw 5G CSV data files. The script scans all raw .csv files using glob, `removes invalid GPS`, `coordinates`, `negative speeds`, `duplicates`, and ***missing critical network metrics*** (e.g., `Bitrate`, `Transfer size`, `etc`.). It **also ensures correct numeric formatting across columns. Cleaned versions of each file are saved individually**, and a combined master file (`cleansed_data.csv`) is exported to my Google Drive for further analysis like clustering or forecasting."],"metadata":{"id":"5HRgfyE5UA2h"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","from glob import glob #file indexing and sorting (neat :D)\n","\n","# Config: Google Drive paths\n","RAW_DATA_PATH = \"/content/drive/MyDrive/COS40007 Design Project/data/\"\n","CLEANED_OUTPUT_PATH = \"/content/drive/MyDrive/COS40007 Design Project/Data Processing/Alvin Version/Processed Data/\"\n","os.makedirs(CLEANED_OUTPUT_PATH, exist_ok=True)\n","\n","# Load a single CSV\n","def load_csv(filepath):\n","    try:\n","        return pd.read_csv(filepath)\n","    except Exception as e:\n","        print(f\"[!] Failed to load {filepath}: {e}\")\n","        return None\n","\n","# Clean a DataFrame\n","def clean_dataframe(df):\n","    required_cols = ['latitude', 'longitude', 'speed', 'Bitrate', 'Bitrate-RX',\n","                     'Transfer size', 'Transfer size-RX', 'CWnd', 'Retransmissions', 'send_data']\n","\n","    # Skip if any required columns are missing\n","    missing = [col for col in required_cols if col not in df.columns]\n","    if missing:\n","        print(f\"[!] Skipped file — missing columns: {missing}\")\n","        return None\n","\n","    # Drop duplicates\n","    df = df.drop_duplicates()\n","\n","    # Filter invalid GPS and speed\n","    df = df[\n","        df['latitude'].between(-38, -10) &\n","        df['longitude'].between(110, 155) &\n","        (df['speed'] >= 0)\n","    ]\n","\n","    # Drop rows with missing critical fields\n","    df = df.dropna(subset=required_cols)\n","\n","    # Convert to numeric to fix type issues\n","    for col in required_cols:\n","        df[col] = pd.to_numeric(df[col], errors='coerce')\n","\n","    # Drop rows with failed conversion\n","    df = df.dropna(subset=required_cols)\n","\n","    return df\n","\n","# Process and merge all CSVs\n","def process_all_files():\n","    files = glob(os.path.join(RAW_DATA_PATH, \"*.csv\"))\n","    print(f\"📦 Found {len(files)} raw CSV files\")\n","\n","    combined_df = []\n","\n","    for file in files:\n","        filename = os.path.basename(file)\n","        df = load_csv(file)\n","        if df is None:\n","            continue\n","        df_clean = clean_dataframe(df)\n","        if df_clean is not None and not df_clean.empty:\n","            combined_df.append(df_clean)\n","            print(f\"[✓] Cleaned: {filename} → {df_clean.shape[0]} rows\")\n","\n","    if combined_df:\n","        final_df = pd.concat(combined_df, ignore_index=True)\n","        final_out = os.path.join(CLEANED_OUTPUT_PATH, \"cleansed_data.csv\")\n","        final_df.to_csv(final_out, index=False)\n","        print(f\"\\n✅ All files cleaned and merged into: cleansed_data.csv\")\n","    else:\n","        print(\"\\n⚠️ No valid data found to merge.\")\n","\n","# Run the script\n","if __name__ == \"__main__\":\n","    process_all_files()\n"],"metadata":{"collapsed":true,"id":"4REE0L-tJU49"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","# Data Engineering"],"metadata":{"id":"m1Q-UEHVU1lC"}},{"cell_type":"markdown","source":["In this step, I developed a complete data engineering pipeline to transform our cleansed 5G network logs into a model-ready dataset named cluster_ready.csv. This engineered dataset is optimized for both unsupervised clustering and supervised forecasting tasks, such as training RandomForest, LightGBM, XGBoost, and LSTM models.\n","\n","The code begins by loading the previously cleaned dataset (cleansed_data.csv) and reconstructs a full timestamp column from the available time parts (Year, Month, Date, hour, min, sec). This enables us to extract time-based patterns essential for modeling.\n","\n","Next, I applied feature engineering in several categories:\n","\n","* Time-based features: Extracted `hour_of_day`, `day_of_week`, and `is_weekend` to capture cyclic and behavioral patterns over time.\n","\n","* Location binning: Rounded `latitude` and `longitude` into `lat_bin` and `lon_bin` to support spatial grouping while avoiding over-granular GPS noise.\n","\n","* Signal strength aggregation: Computed `svr_avg` by averaging `svr1` to `svr4` if available, representing overall network signal quality.\n","\n","* Rolling statistics: Created 5-point moving averages (`rolling_avg`) for key metrics like `Bitrate`, `CWnd`, and `send_data` to smooth out short-term spikes and capture trends.\n","\n","* Delta features: Calculated first-order differences for `Bitrate` and `speed` to highlight sharp changes and transitions in usage or movement.\n","\n","The dataset is then purged of any remaining `NaN` values to ensure model robustness and saved as `cluster_ready.csv`."],"metadata":{"id":"sWyZnuTQZCOW"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","\n","# Paths (Google Drive)\n","INPUT_PATH = \"/content/drive/MyDrive/COS40007 Design Project/Data Processing/Alvin Version/Processed Data/cleansed_data.csv\"\n","OUTPUT_PATH = \"/content/drive/MyDrive/COS40007 Design Project/Data Processing/Alvin Version/Processed Data/cluster_ready.csv\"\n","\n","# Load cleaned data\n","df = pd.read_csv(INPUT_PATH)\n","\n","# Parse timestamp\n","df['timestamp'] = pd.to_datetime(dict(\n","    year=df['Year'],\n","    month=df['Month'],\n","    day=df['Date'],\n","    hour=df['hour'],\n","    minute=df['min'],\n","    second=df['sec']\n","), errors='coerce')\n","df = df.dropna(subset=['timestamp'])\n","\n","# Time features\n","df['hour_of_day'] = df['timestamp'].dt.hour\n","df['day_of_week'] = df['timestamp'].dt.dayofweek\n","df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n","\n","# Location binning\n","df['lat_bin'] = df['latitude'].round(3)\n","df['lon_bin'] = df['longitude'].round(3)\n","\n","# Average signal strength\n","if all(col in df.columns for col in ['svr1', 'svr2', 'svr3', 'svr4']):\n","    df['svr_avg'] = df[['svr1', 'svr2', 'svr3', 'svr4']].mean(axis=1)\n","\n","# Rolling average (5-sample smoothing)\n","rolling_cols = ['Bitrate', 'Bitrate-RX', 'Transfer size', 'Transfer size-RX', 'CWnd', 'send_data']\n","df = df.sort_values('timestamp')\n","for col in rolling_cols:\n","    df[f'{col}_rolling_avg'] = df[col].rolling(window=5, min_periods=1).mean()\n","\n","# Change over time (first-order difference)\n","df['delta_bitrate'] = df['Bitrate'].diff().fillna(0)\n","df['delta_speed'] = df['speed'].diff().fillna(0)\n","\n","# Final cleanup: drop rows with any remaining NaNs\n","df = df.dropna()\n","\n","# Export final engineered dataset\n","df.to_csv(OUTPUT_PATH, index=False)\n","print(\"✅ cluster_ready.csv saved successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pu-8M_6fU6nG","executionInfo":{"status":"ok","timestamp":1746782890481,"user_tz":-600,"elapsed":123257,"user":{"displayName":"Alvin Phan","userId":"12694771152126140622"}},"outputId":"b31fc719-8be5-4a35-90c1-ba6a1ab068f9"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ cluster_ready.csv saved successfully.\n"]}]},{"cell_type":"markdown","source":["### 💡🤡**More to do? -> OPTIMISATION** 👁️👄👁️\n","\n","While current setup is already powerful, further optimizations can be explored:\n","\n","* **Normalization & Scaling:**\n","    Especially important for distance-based clustering (KMeans, DBSCAN) or LSTM input prep.\n","\n","* **Correlation Reduction:**\n","    Dropping redundant features using a correlation matrix can improve model interpretability and efficiency.\n","\n","* **Dimensionality Reduction:**\n","    Applying PCA or t-SNE before clustering can boost performance and visualization.\n","\n","* **Categorical Encoding:**\n","    Convert non-numeric identifiers (e.g., square_id) to one-hot or label-encoded formats if needed.\n","\n","* **Lag Features for LSTM:**\n","    Creating lag-based features (e.g., Bitrate_t-1, Bitrate_t-2) would greatly help sequence-based models."],"metadata":{"id":"Ki64H0PQaA1P"}}]}